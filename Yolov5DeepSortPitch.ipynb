{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ecb52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import warnings\n",
    "import OpCV_Utils\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\MATHEUS DANTAS PEREI\\\\Desktop\\\\DeepSortYOLOv5\\\\DeepSORT_YOLOv5_Pytorch\\\\')\n",
    "sys.path.append('C:\\\\Users\\\\MATHEUS DANTAS PEREI\\\\Desktop\\\\DeepSortYOLOv5\\\\DeepSORT_YOLOv5_Pytorch\\\\deep_sort\\\\deep\\\\checkpoint\\\\')\n",
    "\n",
    "from yolov5.utils.general import (check_img_size, non_max_suppression, scale_coords, xyxy2xywh)\n",
    "from yolov5.utils.torch_utils import select_device, time_synchronized\n",
    "from yolov5.utils.datasets import letterbox\n",
    "\n",
    "from utils_ds.parser import get_config\n",
    "from utils_ds.draw import draw_boxes\n",
    "from deep_sort import build_tracker\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b3a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose Detection Method:\n",
    "def mpPoseDetection(img, pose, mp_pose, mp_drawing, mp_drawing_styles):\n",
    "\n",
    "    # Get results:\n",
    "    results = pose.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Get dims:\n",
    "    H, W, _ = img.shape\n",
    "\n",
    "    if not results.pose_landmarks:\n",
    "        return np.zeros(img.shape, dtype=np.uint8)\n",
    "        \n",
    "    # Create copy:\n",
    "    annotated_img  = np.zeros(img.shape, dtype=np.uint8)\n",
    "    \n",
    "    # Draw pose landmarks on the image.\n",
    "    mp_drawing.draw_landmarks(annotated_img,\n",
    "                              results.pose_landmarks,\n",
    "                              mp_pose.POSE_CONNECTIONS,\n",
    "                              landmark_drawing_spec = mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    \n",
    "    return annotated_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "359a8100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_track(im0, \n",
    "                detector,\n",
    "                deepsort,\n",
    "                img_size,\n",
    "                device,\n",
    "                conf_thres,\n",
    "                iou_thres,\n",
    "                classes):\n",
    "    \"\"\"\n",
    "        Deep Sort Tracking for YOLOv5 Inference method\n",
    "    \"\"\"\n",
    "    \n",
    "    # preprocess ************************************************************\n",
    "    # Padded resize\n",
    "    img = letterbox(im0, new_shape=img_size)[0]\n",
    "    \n",
    "    # Convert:\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "    # numpy to tensor\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    s = '%gx%g ' % img.shape[2:]    # print string\n",
    "\n",
    "    # Detection time *********************************************************\n",
    "    # Inference\n",
    "    t1 = time_synchronized()\n",
    "    with torch.no_grad():\n",
    "        pred = detector(img)[0]  # list: bz * [ (#obj, 6)]\n",
    "\n",
    "    # Apply NMS and filter object other than person (cls:0)\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres,\n",
    "                               classes=classes)\n",
    "    t2 = time_synchronized()\n",
    "\n",
    "    # get all obj ************************************************************\n",
    "    det = pred[0]  # for video, bz is 1\n",
    "    if det is not None and len(det):  # det: (#obj, 6)  x1 y1 x2 y2 conf cls\n",
    "\n",
    "        # Rescale boxes from img_size to original im0 size\n",
    "        det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "        # Print results. statistics of number of each obj\n",
    "        for c in det[:, -1].unique():\n",
    "            n = (det[:, -1] == c).sum()  # detections per class\n",
    "            s += '%g %ss, ' % (n, names[int(c)])  # add to string\n",
    "\n",
    "        bbox_xywh = xyxy2xywh(det[:, :4]).cpu()\n",
    "        confs = det[:, 4:5].cpu()\n",
    "\n",
    "        # ****************************** deepsort ****************************\n",
    "        outputs = deepsort.update(bbox_xywh, confs, im0)\n",
    "        # (#ID, 5) x1,y1,x2,y2,track_ID\n",
    "    else:\n",
    "        outputs = torch.zeros((0, 5))\n",
    "\n",
    "    t3 = time.time()\n",
    "    return outputs, t2-t1, t3-t2\n",
    "\n",
    "###########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0d78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_detector(device, config_deepsort, weights):\n",
    "    \n",
    "    # ***************************** initialize DeepSORT **********************************\n",
    "    cfg = get_config()\n",
    "    cfg.merge_from_file(config_deepsort)\n",
    "\n",
    "    use_cuda = device != 'cpu' and torch.cuda.is_available()\n",
    "    deepsort = build_tracker(cfg, use_cuda=use_cuda)\n",
    "\n",
    "    # ***************************** initialize YOLO-V5 **********************************\n",
    "    detector = torch.load(weights, map_location=device)['model'].float()  # load to FP32\n",
    "    detector.to(device).eval()\n",
    "\n",
    "    names = detector.module.names if hasattr(detector, 'module') else detector.names\n",
    "    \n",
    "    return detector, deepsort, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a99c2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mouseCallback(event, x, y, flags, param):\n",
    "    \n",
    "    global index\n",
    "    global outputs\n",
    "    global resize\n",
    "    global frame_size\n",
    "               \n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        \n",
    "        for detection in outputs:\n",
    "            \n",
    "            # Get bbox and crop it:\n",
    "            x1 = detection[0]\n",
    "            y1 = detection[1]\n",
    "            x2 = detection[2]\n",
    "            y2 = detection[3]\n",
    "                        \n",
    "            xt = 2*int(x*frame_size[1]/resize[0])\n",
    "            yt = 2*int(y*frame_size[0]/resize[1])\n",
    "            \n",
    "            if (x1 < xt < x2) and (y1 < yt < y2):\n",
    "                index = detection[4]\n",
    "                \n",
    "    if event == cv2.EVENT_LBUTTONUP:\n",
    "\n",
    "        for detection in outputs:\n",
    "            \n",
    "            # Get bbox and crop it:\n",
    "            x1 = detection[0]\n",
    "            y1 = detection[1]\n",
    "            x2 = detection[2]\n",
    "            y2 = detection[3]\n",
    "            \n",
    "            xt = 2*int(x*frame_size[1]/resize[0])\n",
    "            yt = 2*int(y*frame_size[0]/resize[1])\n",
    "            \n",
    "            if (x1 < xt < x2) and (y1 < yt < y2):\n",
    "                index = detection[4]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fb43dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MATHEUS DANTAS PEREI\\Desktop\\DeepSortYOLOv5\\DeepSORT_YOLOv5_Pytorch\\utils_ds\\parser.py:23: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  self.update(yaml.load(fo.read()))\n",
      "Loading weights from C:\\Users\\MATHEUS DANTAS PEREI\\Desktop\\DeepSortYOLOv5\\DeepSORT_YOLOv5_Pytorch\\deep_sort\\deep\\checkpoint\\ckpt.t7... Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='NVIDIA GeForce MX450', total_memory=2047MB)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14276\\145833351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;31m# Define output img:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[0mcrop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0mcrop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcrop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpose_det\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m     \u001b[0mcrop_show\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcrop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mcrop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[0moutput_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpose_output\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "# Select Device and define weights and deep sort paths:\n",
    "device  = select_device('0')\n",
    "weights = 'yolov5s6.pt'\n",
    "config_deepsort = 'C:\\\\Users\\\\MATHEUS DANTAS PEREI\\\\Desktop\\\\DeepSortYOLOv5\\\\DeepSORT_YOLOv5_Pytorch\\\\configs\\\\deep_sort.yaml'\n",
    "\n",
    "# Load YOLOv5 model and build detector:\n",
    "detector, deepsort, names = build_detector(device, config_deepsort, weights)\n",
    "\n",
    "# Global YOLOv5 inference parameters:\n",
    "conf_thres = 0.35\n",
    "iou_thres  = 0.10\n",
    "classes    = [0]\n",
    "\n",
    "# Config Mediapipe Pose Estimation Objects:\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing        = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose           = mp.solutions.pose\n",
    "\n",
    "# Config:\n",
    "pose = mp_pose.Pose(static_image_mode        = True,\n",
    "                    model_complexity         = 1,\n",
    "                    enable_segmentation      = False,\n",
    "                    min_detection_confidence = 0.10)\n",
    "\n",
    "# Set which frequency detection models will process frames:\n",
    "frame_interval      = 2\n",
    "frame_interval_pose = 1\n",
    "\n",
    "# Load Video:\n",
    "video_path     = 'football.mp4'\n",
    "cap            = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get text Scale:\n",
    "success, frame = cap.read()\n",
    "img_size       = (max(frame.shape[:2]), (max(frame.shape[:2])))\n",
    "img_size       = (640, 640)\n",
    "text_scale     = max(1, frame.shape[1] // 1600)\n",
    "\n",
    "# Auxiliars:\n",
    "idx_frame = 0\n",
    "last_out  = None\n",
    "yolo_time, sort_time, avg_fps = [], [], []\n",
    "pose_output = np.zeros_like(frame)\n",
    "pose_det    = np.zeros((200, 400)).astype('uint8')\n",
    "crop_show   = np.zeros_like(frame)\n",
    "crop        = np.zeros_like(frame)\n",
    "\n",
    "# Wants Pose Estimation:\n",
    "detect_pose = True\n",
    "\n",
    "# Config Window:\n",
    "cv2.namedWindow('Detection:')\n",
    "cv2.setMouseCallback('Detection:', mouseCallback)\n",
    "resize     = (900, 700)\n",
    "frame_size = frame.shape[:2]\n",
    "\n",
    "index   = -1000\n",
    "outputs = []\n",
    "\n",
    "# Video Loop:\n",
    "while True:\n",
    "    \n",
    "     # Load pitch:\n",
    "    pitch_img = cv2.imread('football_pitch.jpg')\n",
    "    pitch_img = cv2.resize(pitch_img, frame.shape[:2])\n",
    "    pitch_img = cv2.rotate(pitch_img, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "    # Start timer and read video frames:\n",
    "    t0 = time.time()\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    # Check if was read properly:\n",
    "    if success:\n",
    "        \n",
    "        # Make a frame copy and get black board auxiliar:\n",
    "        frame_copy  = frame.copy()\n",
    "\n",
    "    # Check if video ends:\n",
    "    if not success:\n",
    "        \n",
    "        # Repeat video:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        success, frame = cap.read()\n",
    "        \n",
    "        # Make a frame copy and get black board auxiliar:\n",
    "        frame_copy  = frame.copy()\n",
    "\n",
    "    #####################################################################################################\n",
    "    # Apply Deep Sort Track:\n",
    "    if idx_frame % frame_interval == 0:\n",
    "                \n",
    "        outputs, _, _ = image_track(frame, \n",
    "                                      detector,\n",
    "                                      deepsort,\n",
    "                                      img_size,\n",
    "                                      device,\n",
    "                                      conf_thres,\n",
    "                                      iou_thres,\n",
    "                                      classes)\n",
    "        \n",
    "        # Update last output detection:\n",
    "        last_out = outputs\n",
    "    \n",
    "    # Use prediction of the previous frames:\n",
    "    else: outputs = last_out  \n",
    "        \n",
    "    #####################################################################################################\n",
    "    # For each YOLOv5 Detection after Deep Sort, draw bbox and apply pose estimation:\n",
    "    if len(outputs) > 0:\n",
    "\n",
    "        bbox_xyxy  = outputs[:, :4]\n",
    "        identities = outputs[:, -1]\n",
    "        frame = draw_boxes(frame, bbox_xyxy, identities)  # BGR\n",
    "\n",
    "        ##############################################################################################\n",
    "        # Use Mediapipe Pose detection applying to YOLOv5 detected objects:\n",
    "        if idx_frame % 5*frame_interval == 0:\n",
    "            pose_output = np.zeros_like(frame)\n",
    "        \n",
    "        for detection in outputs:\n",
    "            \n",
    "            # Get bbox and crop it:\n",
    "            w    = max(0, detection[2]-detection[0])\n",
    "            h    = max(0, detection[3]-detection[1])\n",
    "            x    = max(0, int(detection[0] - w/2))\n",
    "            y    = max(0, int(detection[1] - h/2))\n",
    "                                    \n",
    "            # Plot pitch:\n",
    "            fx = pitch_img.shape[1]/frame.shape[1]\n",
    "            fy = pitch_img.shape[0]/frame.shape[0]\n",
    "            \n",
    "            cX = int(min(int(detection[0] + w/2), frame.shape[1])*fx)\n",
    "            cY = int(min(int(detection[1] + h/2), frame.shape[0])*fy)\n",
    "                        \n",
    "            cv2.circle(pitch_img, (cX, cY), 5, (255,0,0), thickness = -1, lineType = cv2.LINE_AA)\n",
    "            cv2.putText(pitch_img, f'Player: {detection[4]}',\n",
    "                       (max(0, cX-5), max(0, cY-8)), cv2.FONT_HERSHEY_PLAIN, text_scale, (80, 0, 0), thickness=2)\n",
    "            \n",
    "            if detection[4] == index:\n",
    "                \n",
    "                # Bbox will be twice as big as the original detection:\n",
    "                crop = frame_copy[y:y+2*h, x:x+2*w, :]\n",
    "                cv2.rectangle(frame, (detection[0], detection[1]), (detection[2], detection[3]), [255, 255, 255], 3) \n",
    "                cv2.circle(pitch_img, (cX, cY), 6, (0,0,255), thickness = -1, lineType = cv2.LINE_AA)\n",
    "                \n",
    "                # If frame is correct:\n",
    "                if idx_frame % frame_interval_pose == 0:\n",
    "\n",
    "                    if detect_pose:\n",
    "                        pose_det      = mpPoseDetection(crop, pose, mp_pose, mp_drawing, mp_drawing_styles)\n",
    "                        pose_output[y:y+2*h, x:x+2*w, :] = pose_det\n",
    "\n",
    "                        cv2.putText(pose_output, f'Player: {detection[4]}',\n",
    "                        (x, y), cv2.FONT_HERSHEY_PLAIN, text_scale, (255, 0, 0), thickness=1)\n",
    "                \n",
    "    ##############################################################################################\n",
    "    # Display FPS:\n",
    "    t1 = time.time()\n",
    "    avg_fps.append(t1 - t0)\n",
    "    cv2.putText(frame, f'{len(avg_fps) / sum(avg_fps)}',\n",
    "            (20, 20 + text_scale), cv2.FONT_HERSHEY_PLAIN, text_scale, (0, 0, 255), thickness=2)\n",
    "        \n",
    "    # Keyboard Controls:\n",
    "    key = cv2.waitKey(1) or 0xff   \n",
    "    if key == ord('k'): break\n",
    "    if key == ord('p'): detect_pose = not detect_pose\n",
    "        \n",
    "    # Define output img:\n",
    "    crop = cv2.resize(crop, (200, 400))\n",
    "    crop = np.hstack([crop, cv2.resize(pose_det, (200, 400))])\n",
    "    crop_show[:crop.shape[0], :crop.shape[1],:] = crop\n",
    "    output_img = np.hstack([frame, pose_output])\n",
    "    output_2   = np.hstack([crop_show, cv2.resize(pitch_img, (frame.shape[1], frame.shape[0]))])\n",
    "    output_img = np.vstack([output_img, output_2])\n",
    "    \n",
    "    # Show output:\n",
    "    cv2.imshow('Detection:', cv2.resize(output_img, resize))\n",
    "    \n",
    "    # Update frame counter:\n",
    "    idx_frame += 1\n",
    "    if idx_frame == 3*frame_interval: idx_frame = 0\n",
    "    \n",
    "    # Clear output:\n",
    "    clear_output(wait=False)\n",
    "    \n",
    "#######################################################################################################\n",
    "# Release Video:\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "97ce0a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ecd865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
